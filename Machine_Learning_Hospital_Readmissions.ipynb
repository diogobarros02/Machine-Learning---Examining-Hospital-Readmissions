{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Hospital readmissions represent a significant challenge in the healthcare\n",
    "sector, both as an indicator of care quality and a driver of escalating costs.\n",
    "When a patient is re-admitted to the hospital within a short period after\n",
    "discharge, it not only indicates potential gaps in care but also adds to the\n",
    "financial burden on the healthcare system. In particular, readmissions of\n",
    "diabetic patients have been noted to contribute significantly to these\n",
    "costs. Therefore, being able to predict such readmissions can lead to\n",
    "improved patient care and substantial cost savings.\n",
    "\n",
    "## Dataset Description\n",
    "* **encounter_id:** Unique identifier of the encounter.\n",
    "\n",
    "* **country:** The country in which the encounter took place.\n",
    "\n",
    "* **patient_id:** Identifier of the patient.\n",
    "\n",
    "* **race:** Patient’s race.\n",
    "\n",
    "* **gender:** Patient’s gender.\n",
    "\n",
    "* **age:** Patient’s age bracket.\n",
    "\n",
    "* **weight:** Patient’s weight.\n",
    "\n",
    "* **payer_code:** Code of the health insurance provider (if there is one).\n",
    "\n",
    "* **outpatient_visits_in_previous_year:** Number of outpatient visits (visits made with the intention of leaving on the same day) the patient made to the hospital in the year preceding the encounter.\n",
    "\n",
    "* **emergency_visits_in_previous_year:** Number of emergency visits the patient made to the hospital in the year preceding the encounter.\n",
    "\n",
    "* **inpatient_visits_in_previous_year:** Number of inpatient visits (visits with the intention to stay overnight) the patient made to the hospital in the year preceding the encounter.\n",
    "\n",
    "* **admission_type:** Type of admission of the patient (e.g. Emergency, Urgent, etc.).\n",
    "\n",
    "* **medical_specialty:** Medical specialty on which the patient was admitted.\n",
    "\n",
    "* **average_pulse_bpm:** Average pulse of the patient during their stay in the hospital in beats per minute.\n",
    "\n",
    "* **discharge_disposition:** Destination given to the patient after being discharged.\n",
    "\n",
    "* **admission_source:** Source of the patient before being admitted in the current encounter.\n",
    "\n",
    "* **length_of_stay_in_hospital:** Number of days between admission and discharge.\n",
    "\n",
    "* **number_lab_tests:** Number of lab tests performed during the encounter.\n",
    "\n",
    "* **non_lab_procedures:** Number of non-lab procedures performed during the encounter.\n",
    "\n",
    "* **number_of_medications:** Number of distinct types of medication administered during the encounter.\n",
    "\n",
    "* **primary_diagnosis:** Primary diagnosis (coded as the first three digits of ICD9).\n",
    "\n",
    "* **secondary_diagnosis:** Secondary diagnosis (first three digits of ICD9).\n",
    "\n",
    "* **additional_diagnosis:** Additional secondary diagnosis (first three digits of ICD9).\n",
    "\n",
    "* **number_diagnoses:** Number of diagnoses entered into the system.\n",
    "\n",
    "* **glucose_test_result:** Range of the glucose test results or if the test was not taken. Values: “>200,” “>300,” “normal,” and “none” if not measured.\n",
    "\n",
    "* **a1c_test_result:** Range of the A1C test results or if the test was not taken. Values: “>8” if greater than 8%, “>7” if greater than 7% but less than 8%, “normal” if less than 7%, and “none” if not measured.\n",
    "\n",
    "* **change_in_meds_during_hospitalization:** Indicates if there was a change in diabetic medications (dosage or generic name). Values: “change” and “no change”.\n",
    "\n",
    "* **prescribed_diabetes_meds:** Yes if the patient has diabetes medication prescribed. No otherwise.\n",
    "\n",
    "* **medication:** List containing all generic names for the medications prescribed to the patient during the encounter. An empty list if no medication was prescribed.\n",
    "\n",
    "* **readmitted_binary:** Binary target: Yes if the patient was readmitted in less than 30 days, No otherwise.\n",
    "\n",
    "* **readmitted_multiclass:** Multiclass target: “<30 days” if the patient was readmitted in less than 30 days after being discharged. “>30 days” if the patient was readmitted to the hospital but only after more than 30 days after the current discharge. No otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main decisions to take:** \\\n",
    "Duplicated patient_id \\\n",
    "Missing values \\\n",
    "Outliers \\\n",
    "High cardinality \\\n",
    "Feature engineering \\\n",
    "Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything used just for visualization is commented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"files/train.csv\")\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observe the data, we can see that we have a strange value ('?') in a lot of columns. Let's take a closer look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for col in data.columns:\n",
    "   # print(col + ':', data[col].unique(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for col in data.columns:\n",
    " #   print(col + ':', len(data[col].unique()), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see by the unique values of each column that we have some problems in our dataset:\n",
    "* We will set the index as **encounter_id** as it is the unique identifier\n",
    "* **country** only has one value, so it has no influence to our future predictions. We should drop it\n",
    "* **patient_id** has duplicated values. We will do some feature engineering later based on this\n",
    "* **gender** has a value 'Unknown/Invalid'. We will assume it means the data was missing\n",
    "* **age** and **weight** are categorical variables. We will get to this later \n",
    "* **admission_type** and some other categories have values like 'Not Available' and 'Not Mapped'. We will assume it means the data was missing\n",
    "* **admission_source** has strings with trailing blank spaces\n",
    "* **medical_specialty**, **discharge_disposition**, **admission_source**, **primary_diagnosis**, **secondary_diagnosis**, **additional_diagnosis** and **medication** seem to have a very high number of categories. We will get to this later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set encounter_id as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.set_index('encounter_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "\n",
    "# # Countplot with value counts \n",
    "# sns.countplot(x=\"readmitted_binary\", data=data, ax=axes[0])\n",
    "# axes[0].set_title(\"Distribution of Target Values\")\n",
    "# for p in axes[0].patches:\n",
    "#     axes[0].annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom', xytext=(0, 1), textcoords='offset points')\n",
    "\n",
    "# # Pie chart\n",
    "# data.readmitted_binary.value_counts().plot.pie(autopct=\"%.1f%%\", ax=axes[1])\n",
    "# axes[1].set_title(\"Proportion of Target Value\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we are dealing with an imbalanced dataset. We will get to this later in our modeling. For now we will just encode the target variable as numeric so it can be used later in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['readmitted_binary'] = data['readmitted_binary'].replace({'No': 0, 'Yes': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['country'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove trailing blank spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "data[categorical_columns] = data[categorical_columns].apply(lambda x: x.str.lstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace 'Not Available', 'Not Mapped', '?', 'Unknown/Invalid' with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace(['Not Available', 'Not Mapped', '?', 'Unknown/Invalid'], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we have fixed some of the problems in our data, we can check the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the number of missing values per column\n",
    "# missing_count = data.isnull().sum()\n",
    "\n",
    "# # Calculate the percentage of missing values per column\n",
    "# missing_percentage = (missing_count / len(data) * 100).round(2)\n",
    "\n",
    "# # Create a DataFrame to display the results\n",
    "# missing_info = pd.DataFrame({\n",
    "#     'Missing Count': missing_count,\n",
    "#     'Missing Percentage': missing_percentage\n",
    "# })\n",
    "# # \n",
    "# # Print the missing information\n",
    "# columns_with_missing = missing_info[missing_info['Missing Count'] > 0]\n",
    "# columns_with_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start handling the missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Race**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have about 7% of the data missing, so we can use mode imputation or fill with a placeholder value 'Unknown'. We decided to fill with 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['race'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gender**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have 3 missing values, so we can drop these rows as it won't cause a big loss of information, or we can impute with the mode. We chose to impute with the mode as we don't want to have problems in the future when dealing with the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['gender'].fillna(data['gender'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn the 'age' column into numeric by taking the midpoint of each categorie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping from age groups to numeric values\n",
    "age_mapping = {\n",
    "    '[0-10)': 5,\n",
    "    '[10-20)': 15,\n",
    "    '[20-30)': 25,\n",
    "    '[30-40)': 35,\n",
    "    '[40-50)': 45,\n",
    "    '[50-60)': 55,\n",
    "    '[60-70)': 65,\n",
    "    '[70-80)': 75,\n",
    "    '[80-90)': 85,\n",
    "    '[90-100)': 95\n",
    "}\n",
    "\n",
    "# map values to numerical values\n",
    "data['age'] = data['age'].replace(age_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will check the distribution to see what imputation method is suitable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import kstest\n",
    "\n",
    "# sns.histplot(data['age'], kde=True) \n",
    "# plt.title('Histogram and KDE Plot')\n",
    "# plt.show()\n",
    "\n",
    "# # Perform the Kolmogorov-Smirnov test\n",
    "# stat, p = kstest(data['age'], 'norm')\n",
    "\n",
    "# # Check the p-value to determine normality\n",
    "# if p > 0.05:\n",
    "#     print(\"Data appears to be normally distributed\")\n",
    "# else:\n",
    "#     print(\"Data does not appear to be normally distributed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the information we got the data does not appear to be normally distributed, so a suitable approach would be to impute the missing values with the median or use KNN. We chose to use the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['age'] = data['age'].fillna(data['age'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weight**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a very high percentage of missing values (97%). We can fill the missing values with a placeholder value or we can drop it. We decided to drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['weight'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Payer_code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill with 'Unknown' as it has a very high percentage of missing values (40%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['payer_code'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Glucose_test_result** and **a1c_test_result**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values on both of these columns are not really missing values, they mean that the test was not taken, so we will fill the missing values accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['glucose_test_result'].fillna('Not Taken', inplace=True)\n",
    "data['a1c_test_result'].fillna('Not Taken', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Admission_type**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill with 'Unknown' as it is a suitable approach based on the % of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['admission_type'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Medical_specialty**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill with 'Unknown' as it has a very high percentage of missing values (49%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['medical_specialty'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discharge_disposition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill with 'Unknown' as it is a suitable approach based on the % of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['discharge_disposition'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Admission_source**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill with 'Unknown' as it is a suitable approach based on the % of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['admission_source'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Primary_diagnosis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill with 'Unknown' as it is a suitable approach based on the % of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['primary_diagnosis'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Secondary_diagnosis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill with 'Unknown' as it is a suitable approach based on the % of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['secondary_diagnosis'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional_diagnosis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill with 'Unknown' as it is a suitable approach based on the % of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['additional_diagnosis'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if all the changes were applied and there are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do some data visualization along with trying to reduce the cardinality of the categorical features, in order to reduce the dimensionality. We will also do some feature engineering. Let's check the number of categories in each categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_columns = data.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# # Create an empty list to store DataFrames for each column\n",
    "# dfs = []\n",
    "\n",
    "# # Iterate through each categorical column\n",
    "# for column in categorical_columns:\n",
    "#     num_categories = data[column].nunique()\n",
    "#     df_temp = pd.DataFrame({'Number_of_Categories': [num_categories]}, index=[column])\n",
    "#     dfs.append(df_temp)\n",
    "\n",
    "# # Concatenate the DataFrames into the final result\n",
    "# categories_df = pd.concat(dfs)\n",
    "\n",
    "# # Display the resulting DataFrame\n",
    "# categories_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Race**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 6))\n",
    "\n",
    "\n",
    "# # Countplot with value counts \n",
    "# sns.countplot(x=\"race\", data=data, ax=axes[0])\n",
    "# axes[0].set_title(\"Distribution of Race\")\n",
    "# for p in axes[0].patches:\n",
    "#     axes[0].annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom', xytext=(0, 1), textcoords='offset points')\n",
    "\n",
    "# # Pie chart\n",
    "# data['race'].value_counts().plot.pie(autopct=\"%.1f%%\", ax=axes[1])\n",
    "# axes[1].set_title(\"Proportion of Race\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.catplot(x = 'race', y = 'readmitted_binary', data=data, kind = \"bar\", height= 5, aspect=2)\n",
    "# plt.title(\"Readmitted Probability\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gender**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 6))\n",
    "\n",
    "# # Countplot with value counts \n",
    "# sns.countplot(x=\"gender\", data=data, ax=axes[0])\n",
    "# axes[0].set_title(\"Distribution of Gender\")\n",
    "# for p in axes[0].patches:\n",
    "#     axes[0].annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom', xytext=(0, 1), textcoords='offset points')\n",
    "\n",
    "# # Pie chart\n",
    "# data['gender'].value_counts().plot.pie(autopct=\"%.1f%%\", ax=axes[1])\n",
    "# axes[1].set_title(\"Proportion of Gender\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.catplot(x = 'gender', y = 'readmitted_binary', data=data, kind = \"bar\", height= 5)\n",
    "# plt.title(\"Readmitted Probability\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Payer_code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 6))\n",
    "\n",
    "# # Create a countplot\n",
    "# ax = sns.countplot(x=\"payer_code\", data=data)\n",
    "\n",
    "# # Set labels and title\n",
    "# plt.title(\"Distribution of Payer_code\")\n",
    "# plt.xlabel(\"Payer code\")\n",
    "# plt.ylabel(\"Count\")\n",
    "\n",
    "# # Rotate x-axis labels\n",
    "# ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha='right')\n",
    "\n",
    "# # Annotate each bar with its count\n",
    "# for p in ax.patches:\n",
    "#     ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "#                 ha='center', va='bottom', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grouped data\n",
    "# grouped_data = data.groupby('payer_code')['readmitted_binary'].mean()\n",
    "\n",
    "# # Calculate value counts for each category\n",
    "# value_counts = data['payer_code'].value_counts()\n",
    "\n",
    "# # Sort specialties based on mean values\n",
    "# sorted_data = grouped_data.sort_values(ascending=False)\n",
    "\n",
    "# # Set the figure size using sns\n",
    "# plt.figure(figsize=(20, 6))\n",
    "\n",
    "# # Plot the results with sns\n",
    "# ax = sns.barplot(x=sorted_data.index, y=sorted_data.values)\n",
    "\n",
    "# # Annotate with value counts on top of each bar\n",
    "# for p, label in zip(ax.patches, value_counts.loc[sorted_data.index]):\n",
    "#     ax.annotate(f'{label}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "#                 ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "# plt.title('Probability of Readmission by payer_code with respective value counts')\n",
    "# plt.ylabel('Probability of Readmission')\n",
    "# plt.xlabel('payer_code')\n",
    "# plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no pattern we can see on the data so we will just create a binary column that says if the patient has insurance or not. We will assume the null values we filled with 'Unknown' mean that the patient doesn't have insurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['has_insurance'] = data['payer_code'].map(lambda x: 'No' if x == 'Unknown' else 'Yes')\n",
    "data.drop(columns=['payer_code'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 6))\n",
    "\n",
    "# # Countplot with value counts \n",
    "# sns.countplot(x=\"has_insurance\", data=data, ax=axes[0])\n",
    "# axes[0].set_title(\"Distribution of Has_insurance\")\n",
    "# for p in axes[0].patches:\n",
    "#     axes[0].annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom', xytext=(0, 1), textcoords='offset points')\n",
    "\n",
    "# # Pie chart\n",
    "# data['has_insurance'].value_counts().plot.pie(autopct=\"%.1f%%\", ax=axes[1])\n",
    "# axes[1].set_title(\"Proportion of Has_insurance\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.catplot(x = 'has_insurance', y = 'readmitted_binary', data=data, kind = \"bar\", height= 5)\n",
    "# plt.title(\"Readmitted Probability\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Admission_type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 6))\n",
    "\n",
    "# # Countplot with value counts \n",
    "# sns.countplot(x=\"admission_type\", data=data, ax=axes[0])\n",
    "# axes[0].set_title(\"Distribution of Admission_type\")\n",
    "# for p in axes[0].patches:\n",
    "#     axes[0].annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom', xytext=(0, 1), textcoords='offset points')\n",
    "\n",
    "# # Pie chart\n",
    "# data['admission_type'].value_counts().plot.pie(autopct=\"%.1f%%\", ax=axes[1])\n",
    "# axes[1].set_title(\"Proportion of Admission_type\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.catplot(x = 'admission_type', y = 'readmitted_binary', data=data, kind = \"bar\", height= 5, aspect=2)\n",
    "# plt.title(\"Readmitted Probability\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newborn and Trauma Center categories have very low prevalence in the dataset. We will join 'Trauma Center' with 'Emergency' and 'Newborn' with 'Urgent' because these share similar characteristics in a medical context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_admission_type = {\"Trauma Center\":\"Emergency\",\"Newborn\":\"Urgent\"}\n",
    "data['admission_type'] = data['admission_type'].replace(mapped_admission_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 6))\n",
    "\n",
    "# # Countplot with value counts \n",
    "# sns.countplot(x=\"admission_type\", data=data, ax=axes[0])\n",
    "# axes[0].set_title(\"Distribution of Admission_type\")\n",
    "# for p in axes[0].patches:\n",
    "#     axes[0].annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom', xytext=(0, 1), textcoords='offset points')\n",
    "\n",
    "# # Pie chart\n",
    "# data['admission_type'].value_counts().plot.pie(autopct=\"%.1f%%\", ax=axes[1])\n",
    "# axes[1].set_title(\"Proportion of Admission_type\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.catplot(x = 'admission_type', y = 'readmitted_binary', data=data, kind = \"bar\", height= 5)\n",
    "# plt.title(\"Readmitted Probability\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Medical_specialty**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 6))\n",
    "\n",
    "# # Create a countplot\n",
    "# ax = sns.countplot(x=\"medical_specialty\", data=data)\n",
    "\n",
    "# # Set labels and title\n",
    "# plt.title(\"Distribution of Medical_specialty\")\n",
    "# plt.xlabel(\"Medical Specialty\")\n",
    "# plt.ylabel(\"Count\")\n",
    "\n",
    "# # Rotate x-axis labels\n",
    "# ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha='right')\n",
    "\n",
    "# # Annotate each bar with its count\n",
    "# for p in ax.patches:\n",
    "#     ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "#                 ha='center', va='bottom', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grouped data\n",
    "# grouped_data = data.groupby('medical_specialty')['readmitted_binary'].mean()\n",
    "\n",
    "# # Calculate value counts for each category\n",
    "# value_counts = data['medical_specialty'].value_counts()\n",
    "\n",
    "# # Sort specialties based on mean values\n",
    "# sorted_data = grouped_data.sort_values(ascending=False)\n",
    "\n",
    "# # Set the figure size using sns\n",
    "# plt.figure(figsize=(20, 6))\n",
    "\n",
    "# # Plot the results with sns\n",
    "# ax = sns.barplot(x=sorted_data.index, y=sorted_data.values)\n",
    "\n",
    "# # Annotate with value counts on top of each bar\n",
    "# for p, label in zip(ax.patches, value_counts.loc[sorted_data.index]):\n",
    "#     ax.annotate(f'{label}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "#                 ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "# plt.title('Probability of Readmission by Medical Specialty with respective value counts')\n",
    "# plt.ylabel('Probability of Readmission')\n",
    "# plt.xlabel('Medical Specialty')\n",
    "# plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have a lot of categories. For now we will keep them as is, but we can consider doing some grouping later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discharge_disposition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 6))\n",
    "\n",
    "# # Create a countplot\n",
    "# ax = sns.countplot(x=\"discharge_disposition\", data=data)\n",
    "\n",
    "# # Set labels and title\n",
    "# plt.title(\"Distribution of Discharge_disposition\")\n",
    "# plt.xlabel(\"Discharge disposition\")\n",
    "# plt.ylabel(\"Count\")\n",
    "\n",
    "# # Rotate x-axis labels\n",
    "# ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha='right')\n",
    "\n",
    "# # Annotate each bar with its count\n",
    "# for p in ax.patches:\n",
    "#     ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "#                 ha='center', va='bottom', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grouped data\n",
    "# grouped_data = data.groupby('discharge_disposition')['readmitted_binary'].mean()\n",
    "\n",
    "# # Calculate value counts for each category\n",
    "# value_counts = data['discharge_disposition'].value_counts()\n",
    "\n",
    "# # Sort specialties based on mean values\n",
    "# sorted_data = grouped_data.sort_values(ascending=False)\n",
    "\n",
    "# # Set the figure size using sns\n",
    "# plt.figure(figsize=(20, 6))\n",
    "\n",
    "# # Plot the results with sns\n",
    "# ax = sns.barplot(x=sorted_data.index, y=sorted_data.values)\n",
    "\n",
    "# # Annotate with value counts on top of each bar\n",
    "# for p, label in zip(ax.patches, value_counts.loc[sorted_data.index]):\n",
    "#     ax.annotate(f'{label}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "#                 ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "# plt.title('Probability of Readmission by discharge_disposition with respective value counts')\n",
    "# plt.ylabel('Probability of Readmission')\n",
    "# plt.xlabel('discharge_disposition')\n",
    "# plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything that contains the word Expired means that the patient has died, so he will not be readmitted. Let's join them accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['discharge_disposition'].str.contains(\"expired\", case=False, na=False), 'discharge_disposition'] =  \"Expired\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a lot of categories. For now we will keep them all because the majority of them seem important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Admission_source**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 6))\n",
    "\n",
    "# # Create a countplot\n",
    "# ax = sns.countplot(x=\"admission_source\", data=data)\n",
    "\n",
    "# # Set labels and title\n",
    "# plt.title(\"Distribution of Admission_source\")\n",
    "# plt.xlabel(\"Admission source\")\n",
    "# plt.ylabel(\"Count\")\n",
    "\n",
    "# # Rotate x-axis labels\n",
    "# ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha='right')\n",
    "\n",
    "# # Annotate each bar with its count\n",
    "# for p in ax.patches:\n",
    "#     ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "#                 ha='center', va='bottom', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grouped data\n",
    "# grouped_data = data.groupby('admission_source')['readmitted_binary'].mean()\n",
    "\n",
    "# # Calculate value counts for each category\n",
    "# value_counts = data['admission_source'].value_counts()\n",
    "\n",
    "# # Sort specialties based on mean values\n",
    "# sorted_data = grouped_data.sort_values(ascending=False)\n",
    "\n",
    "# # Set the figure size using sns\n",
    "# plt.figure(figsize=(20, 6))\n",
    "\n",
    "# # Plot the results with sns\n",
    "# ax = sns.barplot(x=sorted_data.index, y=sorted_data.values)\n",
    "\n",
    "# # Annotate with value counts on top of each bar\n",
    "# for p, label in zip(ax.patches, value_counts.loc[sorted_data.index]):\n",
    "#     ax.annotate(f'{label}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "#                 ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "# plt.title('Probability of Readmission by admission_source with respective value counts')\n",
    "# plt.ylabel('Probability of Readmission')\n",
    "# plt.xlabel('admission_source')\n",
    "# plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see we have a lot of categories. To solve this we can try grouping the categories into fewer categories by using the following mapping:\n",
    "* Keep 'Emergency Room'\n",
    "* Group similar ones, like 'Transfer' and 'Referral'\n",
    "* Join the rest into an 'Other' categorie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "admission_source_mapping = {\n",
    "    'Clinic Referral': 'Referral',\n",
    "    'Physician Referral': 'Referral',\n",
    "    'HMO Referral': 'Referral',\n",
    "    'Transfer from another health care facility': 'Transfer',\n",
    "    'Transfer from a hospital': 'Transfer',\n",
    "    'Transfer from a Skilled Nursing Facility (SNF)': 'Transfer',\n",
    "    'Transfer from hospital inpt/same fac reslt in a sep claim': 'Transfer',\n",
    "    'Transfer from critial access hospital': 'Transfer',\n",
    "    'Transfer from Ambulatory Surgery Center': 'Transfer',\n",
    "    'Court/Law Enforcement': 'Other',\n",
    "    'Extramural Birth': 'Other',\n",
    "    'Normal Delivery': 'Other',\n",
    "    'Sick Baby': 'Other',\n",
    "    'Unknown': 'Other'\n",
    "}\n",
    "\n",
    "data['admission_source'] = data['admission_source'].replace(admission_source_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 6))\n",
    "\n",
    "# # Countplot with value counts \n",
    "# sns.countplot(x=\"admission_source\", data=data, ax=axes[0])\n",
    "# axes[0].set_title(\"Distribution of Admission_source\")\n",
    "# for p in axes[0].patches:\n",
    "#     axes[0].annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom', xytext=(0, 1), textcoords='offset points')\n",
    "\n",
    "# # Pie chart\n",
    "# data['admission_source'].value_counts().plot.pie(autopct=\"%.1f%%\", ax=axes[1])\n",
    "# axes[1].set_title(\"Proportion of Admission_source\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.catplot(x = 'admission_source', y = 'readmitted_binary', data=data, kind = \"bar\", height= 5, aspect=2)\n",
    "# plt.title(\"Readmitted Probability\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Primary, secondary and additional diagnosis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing any kind of visualization we have to reduce the number of categories, as these 3 variables have really high cardinality. For this, we will use the mapping based on the ICD9 Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "icd9_mapping = {\n",
    "    '001-139': 'Infectious and parasitic diseases',\n",
    "    '140-239': 'Neoplasms',\n",
    "    '240-279': 'Endocrine, nutritional and metabolic diseases, and immunity disorders',\n",
    "    '280-289': 'Diseases of the blood and blood-forming organs',\n",
    "    '290-319': 'Mental disorders',\n",
    "    '320-389': 'Diseases of the nervous system and sense organs',\n",
    "    '390-459': 'Diseases of the circulatory system',\n",
    "    '460-519': 'Diseases of the respiratory system',\n",
    "    '520-579': 'Diseases of the digestive system',\n",
    "    '580-629': 'Diseases of the genitourinary system',\n",
    "    '630-679': 'Complications of pregnancy, childbirth, and the puerperium',\n",
    "    '680-709': 'Diseases of the skin and subcutaneous tissue',\n",
    "    '710-739': 'Diseases of the musculoskeletal system and connective tissue',\n",
    "    '740-759': 'Congenital anomalies',\n",
    "    '760-779': 'Certain conditions originating in the perinatal period',\n",
    "    '780-799': 'Symptoms, signs, and ill-defined conditions',\n",
    "    '800-999': 'Injury and poisoning',\n",
    "}\n",
    "\n",
    "\n",
    "def is_numeric(input_string):\n",
    "    try:\n",
    "        float(input_string)  # Try to convert the string to a float\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def map_icd9(value):\n",
    "    for key in icd9_mapping:\n",
    "        code_range = key.split('-')\n",
    "        if is_numeric(value):   \n",
    "            if int(code_range[0]) <= float(value) <= int(code_range[1]):\n",
    "                return icd9_mapping[key]\n",
    "        else:\n",
    "            if value.startswith(('E', 'V')):\n",
    "                return 'External causes of injury and supplemental classification'\n",
    "            else:\n",
    "                return 'Unknown_diagnosis'\n",
    "\n",
    "\n",
    "# Apply the mapping primary_diagnosis, secondary_diagnosis and additional_diagnosis \n",
    "data['primary_diagnosis'] = data['primary_diagnosis'].apply(map_icd9)\n",
    "data['secondary_diagnosis'] = data['secondary_diagnosis'].apply(map_icd9)\n",
    "data['additional_diagnosis'] = data['additional_diagnosis'].apply(map_icd9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in [\"primary_diagnosis\",\"secondary_diagnosis\",\"additional_diagnosis\"]:\n",
    "#     plt.figure(figsize=(20, 6))\n",
    "\n",
    "#     # Create a countplot\n",
    "#     ax = sns.countplot(x=col, data=data)\n",
    "\n",
    "#     # Set labels and title\n",
    "#     plt.title(f\"Distribution of {col}\")\n",
    "#     plt.xlabel(col)\n",
    "#     plt.ylabel(\"Count\")\n",
    "\n",
    "#     # Rotate x-axis labels\n",
    "#     ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha='right')\n",
    "\n",
    "#     # Annotate each bar with its count\n",
    "#     for p in ax.patches:\n",
    "#         ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "#                     ha='center', va='bottom', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in [\"primary_diagnosis\",\"secondary_diagnosis\",\"additional_diagnosis\"]:\n",
    "#     sns.catplot(x = col, y = 'readmitted_binary', data=data, kind = \"bar\", height=6, aspect=3)\n",
    "#     plt.title(\"Readmitted Probability\")\n",
    "#     plt.xticks(rotation=90)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Glucose_test_result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "\n",
    "# # Countplot with value counts \n",
    "# sns.countplot(x=\"glucose_test_result\", data=data, ax=axes[0])\n",
    "# axes[0].set_title(\"Distribution of Glucose_test_result\")\n",
    "# for p in axes[0].patches:\n",
    "#     axes[0].annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom', xytext=(0, 1), textcoords='offset points')\n",
    "\n",
    "# # Pie chart\n",
    "# data['glucose_test_result'].value_counts().plot.pie(autopct=\"%.1f%%\", ax=axes[1])\n",
    "# axes[1].set_title(\"Proportion of Glucose_test_result\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.catplot(x = 'glucose_test_result', y = 'readmitted_binary', data=data, kind = \"bar\", height= 5)\n",
    "# plt.title(\"Readmitted Probability\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will turn this into numerical with the following mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "glucose_mapping = {'>300': 3,\n",
    "               '>200': 2,\n",
    "               'Norm': 1,\n",
    "               'Not Taken':0}\n",
    "\n",
    "data['glucose_test_result'] = data['glucose_test_result'] .replace(glucose_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create a new binary column if the patient has or hasn't taken the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "glucose_test_mapping = {3: 'Yes',\n",
    "               2: 'Yes',\n",
    "               1: 'Yes',\n",
    "               0: 'No'}\n",
    "\n",
    "data['glucose_test_taken'] = data['glucose_test_result'].replace(glucose_test_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.catplot(x = 'glucose_test_taken', y = 'readmitted_binary', data=data, kind = \"bar\", height= 5)\n",
    "# plt.title(\"Readmitted Probability\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A1c_test_result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 6))\n",
    "\n",
    "# # Countplot with value counts \n",
    "# sns.countplot(x=\"a1c_test_result\", data=data, ax=axes[0])\n",
    "# axes[0].set_title(\"Distribution of A1c_test_result\")\n",
    "# for p in axes[0].patches:\n",
    "#     axes[0].annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom', xytext=(0, 1), textcoords='offset points')\n",
    "\n",
    "# # Pie chart\n",
    "# data['a1c_test_result'].value_counts().plot.pie(autopct=\"%.1f%%\", ax=axes[1])\n",
    "# axes[1].set_title(\"Proportion of A1c_test_result\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.catplot(x = 'a1c_test_result', y = 'readmitted_binary', data=data, kind = \"bar\", height= 5)\n",
    "# plt.title(\"Readmitted Probability\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some strange information in this catplot. If the a1c is >8 it is considered suboptimal and associated with an higher risk of diabetes-related complications, so we would assume the patients in this range would be more likely to be readmitted, and the same would be true when the a1c is >7, but with a lower risk of diabetes-related complications. As we can see by the catplot the patients in this ranges don't have more probabilities of being readmitted. The patients in the 'Norm' , '>8' and '>7' ranges all have about the same probability of being readmitted (~10%) while the patients who didn't take the test are a little more likely to be readmitted (~11%). Based on this information we will create a new binary variable that represents if the patient has taken the test or not, and turn the original into numerical format as we did with glucose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1c_mapping = {'>7': 2,\n",
    "               '>8': 3,\n",
    "               'Norm': 1,\n",
    "               'Not Taken': 0}\n",
    "\n",
    "data['a1c_test_result'] = data['a1c_test_result'] .replace(a1c_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1c_test_mapping = {3: 'Yes',\n",
    "               2: 'Yes',\n",
    "               1: 'Yes',\n",
    "               0: 'No'}\n",
    "\n",
    "data['a1c_test_taken'] = data['a1c_test_result'] .replace(a1c_test_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.catplot(x = 'a1c_test_taken', y = 'readmitted_binary', data=data, kind = \"bar\", height= 5)\n",
    "# plt.title(\"Readmitted Probability\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change_in_meds_during_hospitalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 6))\n",
    "\n",
    "# # Countplot with value counts \n",
    "# sns.countplot(x=\"change_in_meds_during_hospitalization\", data=data, ax=axes[0])\n",
    "# axes[0].set_title(\"Distribution of Change_in_meds_during_hospitalization\")\n",
    "# for p in axes[0].patches:\n",
    "#     axes[0].annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom', xytext=(0, 1), textcoords='offset points')\n",
    "\n",
    "# # Pie chart\n",
    "# data['change_in_meds_during_hospitalization'].value_counts().plot.pie(autopct=\"%.1f%%\", ax=axes[1])\n",
    "# axes[1].set_title(\"Proportion of Change_in_meds_during_hospitalization\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.catplot(x = 'change_in_meds_during_hospitalization', y = 'readmitted_binary', data=data, kind = \"bar\", height= 5)\n",
    "# plt.title(\"Readmitted Probability\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prescribed_diabetes_meds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 6))\n",
    "\n",
    "# # Countplot with value counts \n",
    "# sns.countplot(x=\"prescribed_diabetes_meds\", data=data, ax=axes[0])\n",
    "# axes[0].set_title(\"Distribution of Prescribed_diabetes_meds\")\n",
    "# for p in axes[0].patches:\n",
    "#     axes[0].annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom', xytext=(0, 1), textcoords='offset points')\n",
    "\n",
    "# # Pie chart\n",
    "# data['prescribed_diabetes_meds'].value_counts().plot.pie(autopct=\"%.1f%%\", ax=axes[1])\n",
    "# axes[1].set_title(\"Proportion of Prescribed_diabetes_meds\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.catplot(x = 'prescribed_diabetes_meds', y = 'readmitted_binary', data=data, kind = \"bar\", height= 5)\n",
    "# plt.title(\"Readmitted Probability\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Medication**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing any kind of visualization we have to reduce the number of categories, as these variable has really high cardinality. To do this we decided to create binary columns for each medication to represent if the medication was or wasn't prescribed. We will also create the following columns:\n",
    "* **num_prescribed_meds** - number of medications prescribed\n",
    "* **prescribed_meds** - if medication was prescribed or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "data['medication'] = data['medication'].apply(ast.literal_eval)\n",
    "\n",
    "# Extract unique medications from the lists\n",
    "unique_medications = set(med for meds in data['medication'] for med in meds)\n",
    "\n",
    "# Create binary columns for each unique medication type\n",
    "for med in unique_medications:\n",
    "    data[med] = data['medication'].apply(lambda x: 'Yes' if med in x else 'No')\n",
    "\n",
    "data['num_prescribed_meds'] = data['medication'].apply(len)\n",
    "data['prescribed_meds'] = data['num_prescribed_meds'].apply(lambda x: 'Yes' if x > 0 else 'No')\n",
    "\n",
    "# drop medication column\n",
    "data.drop(columns=['medication'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "\n",
    "# # Calculate the number of rows needed\n",
    "# num_rows = math.ceil(len(unique_medications) / 3)\n",
    "\n",
    "# # Create subplots with the calculated number of rows and 3 columns\n",
    "# fig, axes = plt.subplots(nrows=num_rows, ncols=3, figsize=(15, 5 * num_rows))\n",
    "\n",
    "# # Flatten the axes array to simplify indexing\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# # Iterate over the columns and create pie charts\n",
    "# for i, column in enumerate(unique_medications):\n",
    "#     data[column].value_counts().plot(kind='pie', autopct='%1.1f%%', ax=axes[i])\n",
    "#     axes[i].set_title(f'Pie Chart for {column}')\n",
    "\n",
    "# # Adjust layout\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Show the plots\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot of columns where one of the classes has very low prevalence. We will drop all the columns where one of the classes has less than 1% prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop =  []\n",
    "\n",
    "for med in unique_medications:\n",
    "    class_1_percentage = ((data[med].value_counts(normalize=True)*100)[1])\n",
    "    if class_1_percentage < 1:\n",
    "        columns_to_drop.append(med)\n",
    "\n",
    "data.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_meds = [x for x in unique_medications if x not in columns_to_drop]\n",
    "\n",
    "# # Calculate the number of rows needed\n",
    "# num_rows = math.ceil(len(new_meds) / 3)\n",
    "\n",
    "# # Create subplots with the calculated number of rows and 3 columns\n",
    "# fig, axes = plt.subplots(nrows=num_rows, ncols=3, figsize=(15, 5 * num_rows))\n",
    "\n",
    "# # Flatten the axes array to simplify indexing\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# # Iterate over the columns and create pie charts\n",
    "# for i, column in enumerate(new_meds):\n",
    "#     data[column].value_counts().plot(kind='pie', autopct='%1.1f%%', ax=axes[i])\n",
    "#     axes[i].set_title(f'Pie Chart for {column}')\n",
    "\n",
    "# # Adjust layout\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Show the plots\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter out columns to drop\n",
    "# new_meds = [x for x in unique_medications if x not in columns_to_drop]\n",
    "\n",
    "# # Calculate the number of rows needed\n",
    "# num_rows = math.ceil(len(new_meds) / 3)\n",
    "\n",
    "# # Create subplots with the calculated number of rows and 3 columns\n",
    "# fig, axes = plt.subplots(nrows=num_rows, ncols=3, figsize=(15, 5 * num_rows))\n",
    "\n",
    "# # Flatten the axes array to simplify indexing\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# # Iterate over the columns and create probability plots\n",
    "# for i, column in enumerate(new_meds):\n",
    "#     sns.barplot(x=column, y='readmitted_binary', data=data, ax=axes[i], ci=None, order=data[column].value_counts().index)\n",
    "#     axes[i].set_title(f'Probability of readmission for {column}')\n",
    "\n",
    "#     # Calculate and display percentages on each bar\n",
    "#     total = len(data[column])\n",
    "#     for p in axes[i].patches:\n",
    "#         percentage = '{:.1f}%'.format(100 * p.get_height())\n",
    "#         x = p.get_x() + p.get_width() / 2\n",
    "#         y = p.get_height()\n",
    "#         axes[i].annotate(percentage, (x, y), ha='center', va='bottom')\n",
    "\n",
    "# # Adjust layout\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Show the plots\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prescribed_meds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 6))\n",
    "\n",
    "# # Countplot with value counts \n",
    "# sns.countplot(x=\"prescribed_meds\", data=data, ax=axes[0])\n",
    "# axes[0].set_title(\"Distribution of Prescribed_Meds\")\n",
    "# for p in axes[0].patches:\n",
    "#     axes[0].annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom', xytext=(0, 1), textcoords='offset points')\n",
    "\n",
    "# # Pie chart\n",
    "# data['prescribed_meds'].value_counts().plot.pie(autopct=\"%.1f%%\", ax=axes[1])\n",
    "# axes[1].set_title(\"Proportion of Prescribed_meds\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.catplot(x = 'prescribed_meds', y = 'readmitted_binary', data=data, kind = \"bar\", height= 5)\n",
    "# plt.title(\"Readmitted Probability\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical features are all ready. Let's do some data visualization in the numerical features and check for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['age', 'outpatient_visits_in_previous_year', 'emergency_visits_in_previous_year', \n",
    "      'inpatient_visits_in_previous_year', 'average_pulse_bpm', 'length_of_stay_in_hospital',\n",
    "      'number_lab_tests','non_lab_procedures', 'number_of_medications', 'number_diagnoses', 'num_prescribed_meds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,6))\n",
    "# correlation_matrix = data[numerical_features].corr()\n",
    "# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "# plt.title('Correlation Heatmap')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some correlation between some of the variables, but nothing to worry about, atleast for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the number of rows needed\n",
    "# num_rows = math.ceil(len(data[numerical_features].columns) / 3)\n",
    "\n",
    "# # Create subplots with the calculated number of rows and 3 columns\n",
    "# fig, axes = plt.subplots(nrows=num_rows, ncols=3, figsize=(20, 5 * num_rows))\n",
    "\n",
    "# # Flatten the axes array to simplify indexing\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# # Iterate over the columns and create histograms\n",
    "# for i, column in enumerate(numerical_features):\n",
    "#     sns.histplot(data[column], bins=30, kde=True, ax=axes[i])\n",
    "#     axes[i].set_title(f'Histogram for {column}')\n",
    "\n",
    "# # Adjust layout\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Show the plots\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the number of rows needed\n",
    "# num_rows = math.ceil(len(data[numerical_features].columns) / 3)\n",
    "\n",
    "# # Create subplots with the calculated number of rows and 3 columns\n",
    "# fig, axes = plt.subplots(nrows=num_rows, ncols=3, figsize=(20, 5 * num_rows))\n",
    "\n",
    "# # Flatten the axes array to simplify indexing\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# # Iterate over the columns and create box plots by class\n",
    "# for i, column in enumerate(numerical_features):  # Exclude the target variable\n",
    "#     sns.boxplot(x='readmitted_binary', y=column, data=data, ax=axes[i])\n",
    "#     axes[i].set_title(f'{column}')\n",
    "\n",
    "# # Adjust layout\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Show the plots\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are outliers in most of the variables. Let's try to get rid of some outliers by applying winsonorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "for col in numerical_features:\n",
    "    data[col] = winsorize(data[col], limits=[0.01, 0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the number of rows needed\n",
    "# num_rows = math.ceil(len(data[numerical_features].columns) / 3)\n",
    "\n",
    "# # Create subplots with the calculated number of rows and 3 columns\n",
    "# fig, axes = plt.subplots(nrows=num_rows, ncols=3, figsize=(20, 5 * num_rows))\n",
    "\n",
    "# # Flatten the axes array to simplify indexing\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# # Iterate over the columns and create histograms\n",
    "# for i, column in enumerate(numerical_features):\n",
    "#     sns.histplot(data[column], bins=30, kde=True, ax=axes[i])\n",
    "#     axes[i].set_title(f'Histogram for {column}')\n",
    "\n",
    "# # Adjust layout\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Show the plots\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the number of rows needed\n",
    "# num_rows = math.ceil(len(data[numerical_features].columns) / 3)\n",
    "\n",
    "# # Create subplots with the calculated number of rows and 3 columns\n",
    "# fig, axes = plt.subplots(nrows=num_rows, ncols=3, figsize=(20, 5 * num_rows))\n",
    "\n",
    "# # Flatten the axes array to simplify indexing\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# # Iterate over the columns and create box plots by class\n",
    "# for i, column in enumerate(numerical_features):  # Exclude the target variable\n",
    "#     sns.boxplot(x='readmitted_binary', y=column, data=data, ax=axes[i])\n",
    "#     axes[i].set_title(f'{column}')\n",
    "\n",
    "# # Adjust layout\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Show the plots\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. We have reduced the number of outliers by a lot. For the rest of the outliers we decided to keep them for now. We might consider binning for some of these numerical variables as some of them have high skewness, but for now we will go like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the following variables: \n",
    "* **patient_dup**  - if the patient is duplicated\n",
    "* **num_patient_dup** - number of times the patient is duplicated\n",
    "* **service_utilization_in_previous_year** - sum of outpatient, emergency and inpatient visits in previous years\n",
    "* **num_readmissions** - number of readmissions\n",
    "* **num_readmissions_in30** - number of readmissions in 30 days\n",
    "* **num_readmissions_over30** - number of readmissions over 30 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save index to use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['patient_dup'] = data['patient_id'].duplicated(keep=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['num_patient_dup'] = data.groupby('patient_id')['patient_id'].transform('count') - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['service_utilization_in_previous_year'] =  data['outpatient_visits_in_previous_year'] + \\\n",
    "                                                data['emergency_visits_in_previous_year'] + data['inpatient_visits_in_previous_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = data[data['readmitted_multiclass'] != 'No'].groupby('patient_id').size().reset_index(name='num_readmissions')\n",
    "\n",
    "data = pd.merge(data, count_df, on='patient_id', how='left')\n",
    "data['num_readmissions'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = data[data['readmitted_multiclass'] == '>30 days'].groupby('patient_id').size().reset_index(name='num_readmissions_over30')\n",
    "\n",
    "data = pd.merge(data, count_df, on='patient_id', how='left')\n",
    "data['num_readmissions_over30'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = data[data['readmitted_multiclass'] == '<30 days'].groupby('patient_id').size().reset_index(name='num_readmissions_in30')\n",
    "\n",
    "data = pd.merge(data, count_df, on='patient_id', how='left')\n",
    "data['num_readmissions_in30'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.set_index(data_index)\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the variables for modeling, we will start by spliiting the target variable from the rest. For now we will not use the following variable **num_readmissions** to prevent data leakage, but we will do some tests with it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['readmitted_binary', 'readmitted_multiclass', 'patient_id', 'num_readmissions', 'num_readmissions_in30', 'num_readmissions_over30'])\n",
    "y = data['readmitted_binary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use label encoder for the categorical variables and power transformer for the numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features.extend(['num_patient_dup', 'service_utilization_in_previous_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "categorical_features = X.select_dtypes(include='object').columns\n",
    "\n",
    "# Create a dictionary to store label encoders\n",
    "label_encoders = {}\n",
    "\n",
    "# Apply label encoding to the training data\n",
    "for feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    X[feature] = le.fit_transform(X[feature].astype(str))\n",
    "    label_encoders[feature] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "pt = PowerTransformer()\n",
    "X[numerical_features] = pt.fit_transform(X[numerical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start with the modeling. First we will create a cross validation function to use in the models. We will use undersampling to balance the dataset. We tried other techniques but this was the best one. We also tried with differente sampling strategies and 1/1.6 was the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import f1_score\n",
    "from statistics import mean\n",
    "\n",
    "def cross_validation(model, X, y, sampler):\n",
    "    scoring_list = []\n",
    "\n",
    "    # Define the cross-validation strategy (StratifiedKFold is used here)\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform cross-validation with undersampling\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Apply undersampling to the training set\n",
    "        X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Train your model on the resampled training set\n",
    "        model.fit(X_resampled, y_resampled)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        predictions = model.predict(X_test)\n",
    "\n",
    "        # Evaluate the model\n",
    "        f1 = f1_score(y_test, predictions)\n",
    "        scoring_list.append(f1)\n",
    "        \n",
    "    model_name = type(model).__name__\n",
    "    print(f'{model_name} CV score:', mean(scoring_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test some models with our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# models = [\n",
    "#     LogisticRegression(random_state=42),\n",
    "#     RandomForestClassifier(random_state=42),\n",
    "#     GradientBoostingClassifier(random_state=42),\n",
    "#     SVC(random_state=42, class_weight='balanced'),\n",
    "#     KNeighborsClassifier(),\n",
    "#     GaussianNB(),\n",
    "#     DecisionTreeClassifier(random_state=42),\n",
    "#     AdaBoostClassifier(random_state=42),\n",
    "#     MLPClassifier(random_state=42),\n",
    "#     HistGradientBoostingClassifier(random_state=42)\n",
    "# ]\n",
    "\n",
    "# rus = RandomUnderSampler(sampling_strategy=1/1.6, random_state=42)\n",
    "\n",
    "# print('RUS')\n",
    "# for model in models:\n",
    "#     cross_validation(model, X, y, rus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see our best model is the GradientBoostingClassifier. We will use GridSearch on it to do some hyper parameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import make_scorer\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# #Set the parameters we want to tune and the values we wants to test\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'max_depth': [3, 5, 7],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "\n",
    "# scorer = make_scorer(f1_score)\n",
    "\n",
    "# undersample = RandomUnderSampler(sampling_strategy=1/1.6, random_state=42)\n",
    "# X_train_balanced, y_train_balanced = undersample.fit_resample(X_train, y_train)\n",
    "\n",
    "# gs = GridSearchCV(GradientBoostingClassifier(), param_grid, cv=3, scoring=scorer, verbose=1, n_jobs=-1)\n",
    "# gs.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# test_f1_score = f1_score(y_test, gs.predict(X_test))\n",
    "# print('F1 score on training set:', gs.best_score_)\n",
    "# print('F1 score on the test set:', test_f1_score)\n",
    "# print(\"Best Parameters:\", gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see Hyperparameter Tuning didn't help, so we will just use the default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier CV score: 0.3565081786453042\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy=1/1.6, random_state=42)\n",
    "\n",
    "gbc = GradientBoostingClassifier(random_state=42)\n",
    "cross_validation(gbc, X, y, rus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a pipeline to prepare the test data for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code commented next was used for kaggle competetion but we figured it would be better to be commented to be less time consuming for the teachers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def initial_preprocessing(data):\n",
    "#     data = data.set_index('encounter_id')\n",
    "#     data.drop(['country'], inplace=True, axis=1)\n",
    "#     categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "#     data[categorical_columns] = data[categorical_columns].apply(lambda x: x.str.lstrip())\n",
    "#     data = data.replace(['Not Available', 'Not Mapped', '?', 'Unknown/Invalid'], np.nan)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def handle_missing_values(data):\n",
    "#     data['race'].fillna('Unknown', inplace=True)\n",
    "#     data['gender'].fillna(data['gender'].mode()[0], inplace=True)\n",
    "\n",
    "#     # Define a mapping from age groups to numeric values\n",
    "#     age_mapping = {\n",
    "#     '[0-10)': 5,\n",
    "#     '[10-20)': 15,\n",
    "#     '[20-30)': 25,\n",
    "#     '[30-40)': 35,\n",
    "#     '[40-50)': 45,\n",
    "#     '[50-60)': 55,\n",
    "#     '[60-70)': 65,\n",
    "#     '[70-80)': 75,\n",
    "#     '[80-90)': 85,\n",
    "#     '[90-100)': 95\n",
    "#     }\n",
    "\n",
    "#     # map values to numerical values\n",
    "#     data['age'] = data['age'].replace(age_mapping)\n",
    "#     data['age'] = data['age'].fillna(data['age'].median())\n",
    "\n",
    "#     data.drop(columns=['weight'], inplace=True)\n",
    "#     data['payer_code'].fillna('Unknown', inplace=True)\n",
    "#     data['glucose_test_result'].fillna('Not Taken', inplace=True)\n",
    "#     data['a1c_test_result'].fillna('Not Taken', inplace=True)\n",
    "#     data['admission_type'].fillna('Unknown', inplace=True)\n",
    "#     data['medical_specialty'].fillna('Unknown', inplace=True)\n",
    "#     data['discharge_disposition'].fillna('Unknown', inplace=True)\n",
    "#     data['admission_source'].fillna('Unknown', inplace=True)\n",
    "#     data['primary_diagnosis'].fillna('Unknown', inplace=True)\n",
    "#     data['secondary_diagnosis'].fillna('Unknown', inplace=True)\n",
    "#     data['additional_diagnosis'].fillna('Unknown', inplace=True)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def handle_high_cardinality(data):\n",
    "#     data['has_insurance'] = data['payer_code'].map(lambda x: 'No' if x == 'Unknown' else 'Yes')\n",
    "#     data.drop(columns=['payer_code'], inplace=True)\n",
    "\n",
    "#     mapped_admission_type = {\"Trauma Center\":\"Emergency\",\"Newborn\":\"Urgent\"}\n",
    "#     data['admission_type'] = data['admission_type'].replace(mapped_admission_type)\n",
    "\n",
    "#     data.loc[data['discharge_disposition'].str.contains(\"expired\", case=False, na=False), 'discharge_disposition'] =  \"Expired\"\n",
    "\n",
    "#     admission_source_mapping = {\n",
    "#         'Clinic Referral': 'Referral',\n",
    "#         'Physician Referral': 'Referral',\n",
    "#         'HMO Referral': 'Referral',\n",
    "#         'Transfer from another health care facility': 'Transfer',\n",
    "#         'Transfer from a hospital': 'Transfer',\n",
    "#         'Transfer from a Skilled Nursing Facility (SNF)': 'Transfer',\n",
    "#         'Transfer from hospital inpt/same fac reslt in a sep claim': 'Transfer',\n",
    "#         'Transfer from critial access hospital': 'Transfer',\n",
    "#         'Transfer from Ambulatory Surgery Center': 'Transfer',\n",
    "#         'Court/Law Enforcement': 'Other',\n",
    "#         'Extramural Birth': 'Other',\n",
    "#         'Normal Delivery': 'Other',\n",
    "#         'Sick Baby': 'Other',\n",
    "#         'Unknown': 'Other'\n",
    "#     }\n",
    "\n",
    "#     data['admission_source'] = data['admission_source'].replace(admission_source_mapping)\n",
    "\n",
    "#     icd9_mapping = {\n",
    "#         '001-139': 'Infectious and parasitic diseases',\n",
    "#         '140-239': 'Neoplasms',\n",
    "#         '240-279': 'Endocrine, nutritional and metabolic diseases, and immunity disorders',\n",
    "#         '280-289': 'Diseases of the blood and blood-forming organs',\n",
    "#         '290-319': 'Mental disorders',\n",
    "#         '320-389': 'Diseases of the nervous system and sense organs',\n",
    "#         '390-459': 'Diseases of the circulatory system',\n",
    "#         '460-519': 'Diseases of the respiratory system',\n",
    "#         '520-579': 'Diseases of the digestive system',\n",
    "#         '580-629': 'Diseases of the genitourinary system',\n",
    "#         '630-679': 'Complications of pregnancy, childbirth, and the puerperium',\n",
    "#         '680-709': 'Diseases of the skin and subcutaneous tissue',\n",
    "#         '710-739': 'Diseases of the musculoskeletal system and connective tissue',\n",
    "#         '740-759': 'Congenital anomalies',\n",
    "#         '760-779': 'Certain conditions originating in the perinatal period',\n",
    "#         '780-799': 'Symptoms, signs, and ill-defined conditions',\n",
    "#         '800-999': 'Injury and poisoning',\n",
    "#     }\n",
    "\n",
    "\n",
    "#     def is_numeric(input_string):\n",
    "#         try:\n",
    "#             float(input_string)  # Try to convert the string to a float\n",
    "#             return True\n",
    "#         except ValueError:\n",
    "#             return False\n",
    "\n",
    "\n",
    "#     def map_icd9(value):\n",
    "#         for key in icd9_mapping:\n",
    "#             code_range = key.split('-')\n",
    "#             if is_numeric(value):   \n",
    "#                 if int(code_range[0]) <= float(value) <= int(code_range[1]):\n",
    "#                     return icd9_mapping[key]\n",
    "#             else:\n",
    "#                 if value.startswith(('E', 'V')):\n",
    "#                     return 'External causes of injury and supplemental classification'\n",
    "#                 else:\n",
    "#                     return 'Unknown_diagnosis'\n",
    "\n",
    "\n",
    "#     data['primary_diagnosis'] = data['primary_diagnosis'].apply(map_icd9)\n",
    "#     data['secondary_diagnosis'] = data['secondary_diagnosis'].apply(map_icd9)\n",
    "#     data['additional_diagnosis'] = data['additional_diagnosis'].apply(map_icd9)\n",
    "\n",
    "#     glucose_mapping = {'>300': 3,\n",
    "#                 '>200': 2,\n",
    "#                 'Norm': 1,\n",
    "#                 'Not Taken': 0}\n",
    "\n",
    "#     data['glucose_test_result'] = data['glucose_test_result'] .replace(glucose_mapping)\n",
    "\n",
    "#     glucose_test_mapping = {3: 'Yes',\n",
    "#                 2: 'Yes',\n",
    "#                 1: 'Yes',\n",
    "#                 0: 'No'}\n",
    "\n",
    "#     data['glucose_test_taken'] = data['glucose_test_result'].replace(glucose_test_mapping)\n",
    "\n",
    "#     a1c_mapping = {'>7': 2,\n",
    "#                 '>8': 3,\n",
    "#                 'Norm': 1,\n",
    "#                 'Not Taken': 0}\n",
    "\n",
    "#     data['a1c_test_result'] = data['a1c_test_result'] .replace(a1c_mapping)\n",
    "\n",
    "#     a1c_test_mapping = {3: 'Yes',\n",
    "#                 2: 'Yes',\n",
    "#                 1: 'Yes',\n",
    "#                 0: 'No'}\n",
    "\n",
    "#     data['a1c_test_taken'] = data['a1c_test_result'] .replace(a1c_test_mapping)\n",
    "\n",
    "\n",
    "\n",
    "#     data['medication'] = data['medication'].apply(ast.literal_eval)\n",
    "\n",
    "#     # Extract unique medications from the lists\n",
    "#     unique_medications = set(med for meds in data['medication'] for med in meds)\n",
    "\n",
    "#     # Create binary columns for each unique medication type\n",
    "#     for med in unique_medications:\n",
    "#         data[med] = data['medication'].apply(lambda x: 'Yes' if med in x else 'No')\n",
    "\n",
    "#     data['num_prescribed_meds'] = data['medication'].apply(len)\n",
    "#     data['prescribed_meds'] = data['num_prescribed_meds'].apply(lambda x: 'Yes' if x > 0 else 'No')\n",
    "    \n",
    "#     # drop medication column\n",
    "#     data.drop(columns=['medication'], inplace=True)\n",
    "\n",
    "#     columns_to_drop =  []\n",
    "\n",
    "#     for med in unique_medications:\n",
    "#         class_1_percentage = ((data[med].value_counts(normalize=True)*100)[1])\n",
    "#         if class_1_percentage < 1:\n",
    "#             columns_to_drop.append(med)\n",
    "\n",
    "#     data.drop(columns=columns_to_drop, inplace=True)\n",
    "#     return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def feature_engineering(data):\n",
    "#     data['num_patient_dup'] = data.groupby('patient_id')['patient_id'].transform('count') - 1\n",
    "#     data['patient_dup'] = data['patient_id'].duplicated(keep=False).astype(int)\n",
    "#     data['service_utilization_in_previous_year'] =  data['outpatient_visits_in_previous_year'] + \\\n",
    "#                                                     data['emergency_visits_in_previous_year'] + data['inpatient_visits_in_previous_year']\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #diogo - \"C:/Users/diogo/Desktop/ML PROJETO/project_data (1)/test.csv\"\n",
    "\n",
    "# test = pd.read_csv(\"files/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = initial_preprocessing(test)\n",
    "# test = handle_missing_values(test)\n",
    "# test = handle_high_cardinality(test)\n",
    "# test = feature_engineering(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's join the patient_dup and num_patient_dup columns from the train to the test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the test index to use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_index = test.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining patient_dup from training with test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def join_patient_dup(data, test):\n",
    "#     pat_mapping = data[['patient_id', 'patient_dup', 'num_patient_dup']]\n",
    "\n",
    "#     common_patients = pat_mapping[pat_mapping['patient_id'].isin(test['patient_id'])]\n",
    "\n",
    "#     duplicated_common_patients = common_patients[common_patients['patient_dup'] == 1]\n",
    "\n",
    "#     test.loc[test['patient_id'].isin(duplicated_common_patients['patient_id']), 'patient_dup'] = 1\n",
    "#     return test\n",
    "\n",
    "# test = join_patient_dup(data, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating num_patient_dup in the test by summing based on the patient_id from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_num_patient_dup(data, test):\n",
    "#     pat_mapping = data[['patient_id', 'patient_dup', 'num_patient_dup']]\n",
    "\n",
    "#     common_patients = pat_mapping[pat_mapping['patient_id'].isin(test['patient_id'])]\n",
    "\n",
    "#     duplicated_common_patients = common_patients[common_patients['patient_dup'] == 1]\n",
    "\n",
    "#     sums_by_patient_id = duplicated_common_patients.groupby('patient_id')['num_patient_dup'].sum().reset_index()\n",
    "\n",
    "#     # Merge num_patient_dup with test DataFrame based on 'patient_id'\n",
    "#     test = pd.merge(test, sums_by_patient_id, on='patient_id', how='left', suffixes=('', '_sum'))\n",
    "\n",
    "#     # Fill NaN values in 'num_patient_dup_sum' with 0\n",
    "#     test['num_patient_dup_sum'].fillna(0, inplace=True)\n",
    "\n",
    "#     # Update 'num_patient_dup' in test\n",
    "#     test['num_patient_dup'] += test['num_patient_dup_sum'].astype(int)\n",
    "\n",
    "#     # Drop the temporary column 'num_patient_dup_sum'\n",
    "#     test.drop('num_patient_dup_sum', axis=1, inplace=True)\n",
    "\n",
    "#     test = test.set_index(test_index)\n",
    "\n",
    "#     return test\n",
    "\n",
    "# test = update_num_patient_dup(data,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace problematic categories with 'Unknown' and drop 'patient_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_med_spec_mapping = {'Perinatology': 'Unknown',\n",
    "#  'Dermatology': 'Unknown',\n",
    "#  'Psychiatry-Addictive': 'Unknown',\n",
    "#  'Surgery-PlasticwithinHeadandNeck': 'Unknown'}\n",
    "\n",
    "# test['medical_specialty'] = test['medical_specialty'].replace(test_med_spec_mapping)\n",
    "# test.drop(columns=['patient_id'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply label encoding and the power transformer to the test data, and right after we order the columns based on the training data columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature in categorical_features:\n",
    "#     le = label_encoders[feature]\n",
    "#     test[feature] = le.transform(test[feature].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt = PowerTransformer()\n",
    "# test[numerical_features] = pt.fit_transform(test[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordered_columns = X.columns\n",
    "# test = test[ordered_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pred = gbc.predict(test)\n",
    "# submit = pd.DataFrame(test_pred, columns=['readmitted_binary'], index=test.index)\n",
    "# submit['readmitted_binary'] = submit['readmitted_binary'].map({0: 'No', 1: 'Yes'})\n",
    "# submit.to_csv('submit.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have got a nice result of 0.3496 in the test set. Now we will test the model with the variables we had dropped earlier to prevent data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = data.drop(columns=['readmitted_binary', 'readmitted_multiclass', 'patient_id'])\n",
    "# y = data['readmitted_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature in categorical_features:\n",
    "#     le = label_encoders[feature]\n",
    "#     X[feature] = le.transform(X[feature].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical_features_2 = numerical_features + ['num_readmissions', 'num_readmissions_in30', 'num_readmissions_over30']\n",
    "\n",
    "# pt = PowerTransformer()\n",
    "# X[numerical_features_2] = pt.fit_transform(X[numerical_features_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test only the best models we got earlier to be more time efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [\n",
    "#     RandomForestClassifier(random_state=42),\n",
    "#     GradientBoostingClassifier(random_state=42),\n",
    "#     AdaBoostClassifier(random_state=42),\n",
    "#     HistGradientBoostingClassifier(random_state=42)\n",
    "# ]\n",
    "\n",
    "\n",
    "# rus = RandomUnderSampler(sampling_strategy=1/1.6, random_state=42)\n",
    "\n",
    "# for model in models:\n",
    "#     cross_validation(model, X, y, rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# gb_classifier = HistGradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# undersample = RandomUnderSampler(sampling_strategy=1/1.6) \n",
    "# X_train_balanced, y_train_balanced = undersample.fit_resample(X_train, y_train)\n",
    "\n",
    "# # Fit the model to the training data\n",
    "# gb_classifier.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = gb_classifier.predict(X_test)\n",
    "\n",
    "# print('Classification Report:')\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create these variables in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv(\"files/test.csv\")\n",
    "# test = initial_preprocessing(test)\n",
    "# test = handle_missing_values(test)\n",
    "# test = handle_high_cardinality(test)\n",
    "# test = feature_engineering(test)\n",
    "\n",
    "# test_index = test.index\n",
    "# test = join_patient_dup(data, test)\n",
    "# test = update_num_patient_dup(data,test)\n",
    "# test['medical_specialty'] = test['medical_specialty'].replace(test_med_spec_mapping)\n",
    "\n",
    "# for feature in categorical_features:\n",
    "#     le = label_encoders[feature]\n",
    "#     test[feature] = le.transform(test[feature].astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_patients = data[data['patient_id'].isin(test['patient_id'])]\n",
    "# patient_id_to_readmissions = dict(zip(common_patients['patient_id'], common_patients['num_readmissions']))\n",
    "# test['num_readmissions'] = test['patient_id'].map(patient_id_to_readmissions)\n",
    "# test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_patients = data[data['patient_id'].isin(test['patient_id'])]\n",
    "# patient_id_to_readmissions = dict(zip(common_patients['patient_id'], common_patients['num_readmissions_in30']))\n",
    "# test['num_readmissions_in30'] = test['patient_id'].map(patient_id_to_readmissions)\n",
    "# test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_patients = data[data['patient_id'].isin(test['patient_id'])]\n",
    "# patient_id_to_readmissions = dict(zip(common_patients['patient_id'], common_patients['num_readmissions_over30']))\n",
    "# test['num_readmissions_over30'] = test['patient_id'].map(patient_id_to_readmissions)\n",
    "# test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.drop(columns=['patient_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt = PowerTransformer()\n",
    "# test[numerical_features_2] = pt.fit_transform(test[numerical_features_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordered_columns = X.columns\n",
    "# test = test[ordered_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pred = gb_classifier.predict(test)\n",
    "# submit = pd.DataFrame(test_pred, columns=['readmitted_binary'], index=test.index)\n",
    "# submit['readmitted_binary'] = submit['readmitted_binary'].map({0: 'No', 1: 'Yes'})\n",
    "# submit.to_csv('submit.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got very good scores on the training and validations sets when we tried the new variables, but in the test sets the results were a lot worse than what we had. This is probably due to the data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiclass**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=data.copy()\n",
    "numerical_features2 = numerical_features\n",
    "categorical_features2 = categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['readmitted_multiclass'] = data2['readmitted_multiclass'].replace({'No': 0, '>30 days': 1, '<30 days': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aditional preprocessing in multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Replace 'your_dataframe' with your DataFrame variable\n",
    "# # and 'target_column' with your target column name.\n",
    "\n",
    "# # Identifying categorical columns (excluding the target column)\n",
    "# categorical_columns = data2.select_dtypes(include=['object', 'category']).columns\n",
    "# categorical_columns = [col for col in categorical_columns if col != 'readmitted_multiclass']\n",
    "\n",
    "# # Creating plots for each categorical feature\n",
    "# for column in categorical_columns:\n",
    "#     # Counting occurrences and normalizing to get probabilities\n",
    "#     count_df = data2.groupby([column, 'readmitted_multiclass']).size().unstack(fill_value=0)\n",
    "#     prob_df = count_df.div(count_df.sum(axis=1), axis=0)\n",
    "\n",
    "#     # Plotting\n",
    "#     prob_df.plot(kind='bar', stacked=True, figsize=(10,6))\n",
    "#     plt.xlabel(column.capitalize())\n",
    "#     plt.ylabel('Probability')\n",
    "#     plt.title(f'Probability of Target Variable for Each {column.capitalize()}')\n",
    "#     plt.legend(title='Target', labels=['0', '1', '2'])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " as we can see, a lot of features dont show a relevcancy for thr predictions because the target variable distribution is roughly the same across different groups of the feature, it may suggest that some features dont have a strong predictive power for the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # and 'target_column' with your target column name.\n",
    "# import pandas as pd\n",
    "# from scipy.stats import chi2_contingency\n",
    "# df = data2\n",
    "# target_column = 'readmitted_multiclass'\n",
    "\n",
    "# # List to keep track of p-values\n",
    "# chi_squared_results = []\n",
    "\n",
    "# # Iterating over each categorical feature\n",
    "# for column in df.select_dtypes(include=['object', 'category']).columns:\n",
    "#     if column != target_column:\n",
    "#         # Creating a contingency table\n",
    "#         contingency_table = pd.crosstab(df[column], df[target_column])\n",
    "\n",
    "#         # Chi-squared test\n",
    "#         chi2, p, dof, ex = chi2_contingency(contingency_table, correction=False)\n",
    "        \n",
    "#         # Append the result to the list\n",
    "#         chi_squared_results.append((column, chi2, p))\n",
    "\n",
    "# # Creating a DataFrame to display results\n",
    "# chi_squared_df = pd.DataFrame(chi_squared_results, columns=['Feature', 'Chi2 Statistic', 'P-value'])\n",
    "\n",
    "# # Sorting by p-value\n",
    "# chi_squared_df = chi_squared_df.sort_values(by='P-value')\n",
    "\n",
    "# print(chi_squared_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we can drop the ones with higher than 0.05 p-value bc they are not statistically significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.drop(columns = ['glyburide'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features2 = [col for col in categorical_features2 if col not in ['glyburide']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation_matrix = data2.corr()\n",
    "\n",
    "# # Select correlations of the target variable with other features\n",
    "# target_correlation = correlation_matrix['readmitted_multiclass']\n",
    "\n",
    "# # If you want to drop the target variable's self-correlation\n",
    "# target_correlation = target_correlation.drop(labels=['readmitted_multiclass'])\n",
    "# print(\"\\nCorrelation of Features with the Target Variable (excluding self-correlation):\")\n",
    "# print(target_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maybe drop average_pulse_bpm bc its very very low related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.drop(columns = ['average_pulse_bpm'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features2 = [col for col in numerical_features2 if col not in ['average_pulse_bpm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Set the target variable\n",
    "# target_variable = 'readmitted_multiclass'\n",
    "\n",
    "# # Loop through each column and create a violin plot\n",
    "# for column in data2[numerical_features2].columns:\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.violinplot(x=target_variable, y=column, data=df)\n",
    "#     plt.title(f'Violin Plot of {column} by {target_variable}')\n",
    "#     plt.xlabel(target_variable)\n",
    "#     plt.ylabel(column)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it didnt do anything good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Variable for which you want to calculate the percentages\n",
    "# variable = 'num_patient_dup'  # Replace with the actual name of your variable\n",
    "\n",
    "# # Loop through each unique value in the variable\n",
    "# for unique_value in data2[variable].unique():\n",
    "#     # Filter the DataFrame for the current unique value\n",
    "#     filtered_df = data2[data2[variable] == unique_value]\n",
    "\n",
    "#     # Calculate the percentage of 0, 1, and 2 in the target variable for this subset\n",
    "#     value_counts = filtered_df['readmitted_multiclass'].value_counts(normalize=True) * 100  # Replace 'Target' with your target variable name\n",
    "\n",
    "#     # Print the percentages\n",
    "#     print(f'For {variable} = {unique_value}:')\n",
    "#     for value in [0, 1, 2]:\n",
    "#         percentage = value_counts.get(value, 0)  # Get the percentage for each value, defaulting to 0 if not found\n",
    "#         print(f'  Percentage of {value} in Target: {percentage:.2f}%')\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "didnt take anything from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column in data2[numerical_features2].columns:\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     sns.histplot(data2[column], kde=True)\n",
    "#     plt.title(f'Distribution of {column}')\n",
    "#     plt.xlabel(column)\n",
    "#     plt.ylabel('Frequency')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "applied box-cox transformations for the skewed distributed and with no 0's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              length_of_stay_in_hospital  number_of_medications  \\\n",
      "encounter_id                                                      \n",
      "533253                                 2                     20   \n",
      "426224                                14                     25   \n",
      "634063                                 6                     22   \n",
      "890610                                 6                      9   \n",
      "654194                                 6                     15   \n",
      "\n",
      "              boxcox_length_of_stay_in_hospital  boxcox_number_of_medications  \n",
      "encounter_id                                                                   \n",
      "533253                                 0.725748                      5.037858  \n",
      "426224                                 3.155394                      5.643808  \n",
      "634063                                 2.020663                      5.291367  \n",
      "890610                                 2.020663                      3.193172  \n",
      "654194                                 2.020663                      4.318121  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "# Columns to transform\n",
    "columns_to_transform = ['length_of_stay_in_hospital', 'number_of_medications']  # Replace with actual column names\n",
    "\n",
    "# Loop through each column and apply Box-Cox transformation\n",
    "for column in columns_to_transform:\n",
    "    # Check if the column contains zero or negative values\n",
    "    if data2[column].min() <= 0:\n",
    "        # Shift the column values to be strictly positive\n",
    "        data2[column] += (1 - data2[column].min())\n",
    "\n",
    "    # Apply Box-Cox transformation\n",
    "    data2[f'boxcox_{column}'], _ = stats.boxcox(data2[column])\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify the transformation\n",
    "print(data2[[*columns_to_transform, *[f'boxcox_{col}' for col in columns_to_transform]]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features2.extend(['boxcox_length_of_stay_in_hospital',  'boxcox_number_of_medications'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features2 = [col for col in numerical_features2 if col not in ['length_of_stay_in_hospital','number_of_medications']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column in ['boxcox_length_of_stay_in_hospital',  'boxcox_number_of_medications']:\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     sns.histplot(data2[column], kde=True)\n",
    "#     plt.title(f'Distribution of {column}')\n",
    "#     plt.xlabel(column)\n",
    "#     plt.ylabel('Frequency')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data2.drop(columns=['readmitted_binary', 'readmitted_multiclass', 'patient_id', 'num_readmissions', 'num_readmissions_in30', 'num_readmissions_over30'])\n",
    "y = data2['readmitted_multiclass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in categorical_features2:\n",
    "    le = LabelEncoder()\n",
    "    X[feature] = le.fit_transform(X[feature].astype(str))\n",
    "    label_encoders[feature] = le\n",
    "\n",
    "\n",
    "pt = PowerTransformer()\n",
    "X[numerical_features2] = pt.fit_transform(X[numerical_features2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Instantiate the RandomUnderSampler\n",
    "undersampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFE CV \n",
    "- since running rfecv takes a lot of time we just created a variable with the features that were choosen by the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator = GradientBoostingClassifier()  # or another classifier\n",
    "\n",
    "# from sklearn.feature_selection import RFECV\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# # Create the RFECV object, using a stratified k-fold cross-validation\n",
    "# rfecv = RFECV(estimator, stp=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
    "\n",
    "# # Fit RFECV to the training data\n",
    "# rfecv.fit(X_train, y_train)\n",
    "\n",
    "# # The mask of selected features can be obtained with `rfecv.support_`\n",
    "# selected_features = X_train.columns[rfecv.support_]\n",
    "\n",
    "# # Print the number of features selected and their names\n",
    "# print(f\"Optimal number of features: {rfecv.n_features_}\")\n",
    "# print(f\"Selected features: {selected_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['race', 'age', 'outpatient_visits_in_previous_year',\n",
    "       'emergency_visits_in_previous_year',\n",
    "       'inpatient_visits_in_previous_year', 'admission_type',\n",
    "       'medical_specialty', 'discharge_disposition', 'admission_source',\n",
    "       'length_of_stay_in_hospital', 'number_lab_tests',\n",
    "       'number_of_medications', 'primary_diagnosis', 'secondary_diagnosis',\n",
    "       'additional_diagnosis', 'number_diagnoses', 'has_insurance',\n",
    "       'glucose_test_taken', 'num_prescribed_meds', 'patient_dup',\n",
    "       'num_patient_dup', 'service_utilization_in_previous_year',\n",
    "       'boxcox_number_of_medications']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[selected_features]\n",
    "X_test = X_test[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.80      0.78      7681\n",
      "           1       0.57      0.35      0.43      4977\n",
      "           2       0.25      0.51      0.34      1590\n",
      "\n",
      "    accuracy                           0.61     14248\n",
      "   macro avg       0.53      0.55      0.52     14248\n",
      "weighted avg       0.64      0.61      0.61     14248\n",
      "\n"
     ]
    }
   ],
   "source": [
    " #Instantiate the RandomUnderSampler\n",
    "undersampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Replace RandomForestClassifier with your chosen model\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# Train the model on the resampled training set\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a accuracy score of 0.61, which is acceptable. We could have gotten a higher accuracy score if we didn't balance the classes but that would just assing the predictions to the majority class, and that wouldn't be considered a useful model. So we focused having a good recall, precision and f1 score in the minority classes. Now we will do cross validation to validate our results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### the next one is without undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Replace RandomForestClassifier with your chosen model\n",
    "# model = GradientBoostingClassifier()\n",
    "\n",
    "# # Train the model on the resampled training set\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Print classification report\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from statistics import mean\n",
    "\n",
    "# def cross_validation_mc(model, X, y, sampler):\n",
    "#     scoring_list = []\n",
    "\n",
    "#     # Define the cross-validation strategy (StratifiedKFold is used here)\n",
    "#     cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "#     # Perform cross-validation with undersampling\n",
    "#     for train_index, test_index in cv.split(X, y):\n",
    "#         X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "#         y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "#         # Apply undersampling to the training set\n",
    "#         X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "#         # Train your model on the resampled training set\n",
    "#         model.fit(X_resampled, y_resampled)\n",
    "\n",
    "#         # Make predictions on the test set\n",
    "#         predictions = model.predict(X_test)\n",
    "\n",
    "#         # Evaluate the model using accuracy\n",
    "#         accuracy = accuracy_score(y_test, predictions)\n",
    "#         scoring_list.append(accuracy)\n",
    "\n",
    "#     model_name = type(model).__name__\n",
    "#     print(f'{model_name} CV accuracy:', mean(scoring_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "# from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# models = [\n",
    "#     RandomForestClassifier(random_state=42),\n",
    "#     GradientBoostingClassifier(random_state=42),\n",
    "#     AdaBoostClassifier(random_state=42),\n",
    "#     HistGradientBoostingClassifier(random_state=42)\n",
    "# ]\n",
    "\n",
    "# rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# print('RUS')\n",
    "# for model in models:\n",
    "#     cross_validation_mc(model, X, y, rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
